from nltk.tokenize import word_tokenize
from collections import Counter
from torch.utils.data import Dataset
import torch
import pickle
from matplotlib import pyplot as plt


class SkipgramData(Dataset):
    """
    A dataset class for generating training data for Skip-gram models from a corpus.

    It supports plotting the frequency distribution of words in the corpus if desired.

    :param corpus_file: Path to the text corpus file.
    :param vocab_size: The number of most common words to consider in the vocabulary.
    :param window_size: The size of the context window around each target word.
    :param plt: A boolean flag indicating whether to plot the word frequency distribution.

    Attributes:
        vocab_size (int): The vocabulary size.
        window_size (int): The context window size.
        plt (bool): Flag to plot word frequency distribution.
        tokens (list): List of tokens extracted from the corpus.
        word2id (dict): Mapping from words to their respective IDs.
        id2word (dict): Mapping from word IDs to their respective words.
        train_data (list): The generated training data, each element being a tuple (target_id, context_id, label).
    """
    def __init__(self, corpus_file, vocab_size, window_size, plt=False):
        self.vocab_size = vocab_size
        self.window_size = window_size
        self.plt = plt
        
        with open(corpus_file, "r") as f:
            text = f.read().lower()
        
        self.tokens = word_tokenize(text)
        word_count = Counter(self.tokens)
        if self.plt:
            plot_freq(word_count)
        self.word2id = {word: i for i, (word, _) in enumerate(word_count.most_common(vocab_size))}
        self.id2word = {i: word for word, i in self.word2id.items()} 
        self.train_data = self.gen_train_data()
    
    def gen_train_data(self):
        """
        Generates training data samples from the tokenized corpus.

        This method iterates over each token in the corpus, using it as a target word to gather context words 
        within the specified window size. For each target-context pair, a positive sample is created. 
        Additionally, for each target word, negative samples are generated by randomly selecting words from the 
        vocabulary that are not in the target's context.

        :return: A list of training samples, where each sample is a tuple (target_id, context_id, label).
        """
        train_data = []
        for i, target in enumerate(self.tokens):
            target_id = self.word2id.get(target, None)
            if target_id is None:
                continue
            for j in range(max(0, i - self.window_size), min(len(self.tokens), i + self.window_size)):
                if i != j:
                    context = self.tokens[j]
                    context_id = self.word2id.get(context, None)
                    if context_id is None:
                        continue
                    train_data.append((target_id, context_id, 1))
                    
                    for _ in range(self.window_size):
                        find = False
                        while not find:
                            negative = self.id2word.get(torch.randint(0, self.vocab_size, (1,)).item())
                            if negative_id != target_id:
                                find = False
                        negative_id = self.word2id.get(negative, None)
                        if negative_id is None:
                            continue
                        train_data.append((target_id, negative_id, 0))
        return train_data
        
    def __len__(self):
        return len(self.train_data)
    
    def __getitem__(self, idx):
        target_id, context_id, label = self.train_data[idx]
        
        return (
            torch.tensor(target_id, dtype=torch.long), 
            torch.tensor(context_id, dtype=torch.long),
            torch.tensor(label, dtype=torch.float)
        )
    
    def save_dataset(self, save_path):
        with open(save_path, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load_dataset(cls, load_path):
        with open(load_path, 'rb') as f:
            dataset = pickle.load(f)
        return dataset


def plot_freq(word_count):
    """
    Plots the frequency distribution of words in the corpus.

    :param word_count: A Counter object containing word frequencies.
    """
    word_freq = sorted(word_count.values(), reverse=True)
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(word_freq) + 1), word_freq, marker='o', linestyle='-')
    plt.xlabel("Rank")
    plt.ylabel("Frequencies")
    plt.title("Word Frequency Distribution")
    plt.tight_layout()
    plt.show()