import torch
from matplotlib import pyplot as plt
from .base_dataset import BaseData


class SkipgramData(BaseData):
    """
    A dataset class for generating training data for Skip-gram models from a corpus.

    It supports plotting the frequency distribution of words in the corpus if desired.

    :param corpus_file: Path to the text corpus file.
    :param vocab_size: The number of most common words to consider in the vocabulary.
    :param window_size: The size of the context window around each target word.
    :param negative_size: The size of the negative sample per context word.
    :param sample_size: The number of corpus to sample.

    Attributes:
        vocab_size (int): The vocabulary size.
        window_size (int): The context window size.
        negative_size (int): The size of the negative sample per context word.
        corpora (List): A list of list of tokens from each corpus
        word2id (dict): Mapping from words to their respective IDs.
        id2word (dict): Mapping from word IDs to their respective words.
        train_data (list): The generated training data, each element being a tuple (target_id, context_id, label).
    """
    def __init__(self, corpus_file, vocab_size, window_size, negative_size, sample_size=None):
        super().__init__(corpus_file, vocab_size, window_size, negative_size, sample_size)
        
        # process data in corpus level
        for corpus in self.corpora:
            self.train_data.extend(self._gen_train_data(corpus))
    
    def _gen_train_data(self, corpus):
        """
        Generates training data samples from the tokenized corpus.

        This method iterates over each token in the corpus, using it as a target word to gather context words 
        within the specified window size. For each target-context pair, a positive sample is created. 
        Additionally, for each target word, negative samples are generated by randomly selecting words from the 
        vocabulary that are not in the target's context.

        :param corpus: A list of tokens for the corpus.
        :return: A list of training samples, where each sample is a tuple (target_id, context_id, label).
        """
        train_data = []
        for i, target in enumerate(corpus):
            target_id = self.word2id.get(target, None)
            if target_id is None:
                continue
            for j in range(max(0, i - self.window_size), min(len(corpus), i + self.window_size + 1)):
                if i != j:
                    context = corpus[j]
                    context_id = self.word2id.get(context, None)
                    if context_id is None:
                        continue
                    train_data.append((target_id, context_id, 1))
                    
                    negative_samples = set()
                    while len(negative_samples) < self.negative_size:
                        negative_id = torch.randint(0, self.vocab_size, (1,)).item()
                        if negative_id != target_id and negative_id not in negative_samples:
                            negative_samples.add(negative_id)
                            train_data.append((target_id, negative_id, 0)) 
        return train_data
    
    def __getitem__(self, idx):
        target_id, context_id, label = self.train_data[idx]
        
        return (
            torch.tensor(target_id, dtype=torch.long), 
            torch.tensor(context_id, dtype=torch.long),
            torch.tensor(label, dtype=torch.float)
        )


def plot_freq(word_count):
    """
    Plots the frequency distribution of words in the corpus.

    :param word_count: A Counter object containing word frequencies.
    """
    word_freq = sorted(word_count.values(), reverse=True)
    plt.figure(figsize=(12, 6))
    plt.plot(range(1, len(word_freq) + 1), word_freq, marker='o', linestyle='-')
    plt.xlabel("Rank")
    plt.ylabel("Frequencies")
    plt.title("Word Frequency Distribution")
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    file = "data/EngOrdText"
    vocab_size = 1000
    window_size = 5
    negative_size = 5
    sample_size = 500

    dataset = SkipgramData(file, vocab_size, window_size, negative_size, sample_size)
    dataset.save_dataset("outputs/test_norman_skipgram_data")